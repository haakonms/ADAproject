{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter trolls - how Russia meddles with western democracies\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Over the last several years there has been an attempt from Russian trolls to spread propaganda and fake news over social media in order to spread political ideas among the general population both nationally and internationally. Can these attempts be regarded as undermining the democracy of the affected countries?\n",
    "\n",
    "In this project we are going to analyze a great number of these russian tweets and look into their motivations for this meddling. We will mainly look into their overall political goals in the US, and examine how these goals change over time. Have the trolls achieved their goals? We will also look into how the trolls operate and organize themselves, trying to find patterns in the madness. Such patterns can hopefully help the general population to indicate that a tweet is originating from a troll. As the Russian efforts are increasing every year, a solution is needed to defend the democracy.\n",
    "\n",
    "### Imports used in this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import scipy as sp\n",
    "from pyspark.sql import *\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median \n",
    "from langdetect import detect\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from matplotlib.pyplot import figure\n",
    "#import pyspark.sql.SQLContext\n",
    "from ipywidgets import *\n",
    "import pyspark.sql.functions as func\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Want to ignore this since we had the same error running over and over regarding to whether use loc or make a copy. In our notebook the difference does not matter.\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Preprocessing the data\n",
    "The data sets are of different shape and quality. The first one, provided by two researchers from Clemson University has already been preprocessed a bit. The second one is still quite raw, even though it has certain features as for example language. We have been cleaning some of it, and also processing and enriching to make new data frames that we want to use in our analysis.\n",
    "\n",
    "## 1.1) The first dataset - IRA tweets from Kaggle.\n",
    "The first dataset has the following features:\n",
    "- external_author_id | An author account ID from Twitter \n",
    "- author | The handle sending the tweet\n",
    "- content | The text of the tweet\n",
    "- region | A region classification based on following criterias:\n",
    "     - 1. Geographical coordinates\n",
    "     - 2. Location listed in the user's profile\n",
    "     - 3. Time zone set by the user in their Twitter account settings.\n",
    "- language | The language of the tweet\n",
    "- publish_date | The date and time the tweet was sent\n",
    "- harvested_date | The date and time the tweet was collected by Social Studio\n",
    "- following | The number of accounts the handle was following at the time of the tweet\n",
    "- followers | The number of followers the handle had at the time of the tweet\n",
    "- updates | The number of “update actions” on the account that authored the tweet, including tweets, retweets and likes\n",
    "- post_type | Indicates if the tweet was a retweet or a quote-tweet\n",
    "- account_type | Specific account theme, as coded by Linvill and Warren\n",
    "- retweet | A binary indicator of whether or not the tweet is a retweet\n",
    "- account_category` | General account theme, as coded by Linvill and Warren\n",
    "- new_june_2018 | A binary indicator of whether the handle was newly listed in June 2018\n",
    "\n",
    "\n",
    "### 1.1.1) Loading the first dataset\n",
    "We loaded the 9 files into a unified dataframe. We decided to use pandas, since the dataset is not too large, and especially with the fact that we will be focusing on subsets of these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile('russian-troll-tweets.zip')\n",
    "data = pd.DataFrame()\n",
    "for i in range(1,9):\n",
    "    data = data.append(pd.read_csv(zip_file.open(\"IRAhandle_tweets_\"+str(i)+\".csv\")))\n",
    "first_set_data=data.reset_index()\n",
    "first_set_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_set_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Cleaning and filtering the first dataset\n",
    "The first dataset is pretty clean as it is, but it contains some nan-values and duplicates. We are also working with some parts of the data set, so it's filtered a bit as well.\n",
    "\n",
    "#### 1.1.2.1) Dropping the data we don't want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index column is unnecessary as pandas assigns an index anyway. \"new_june_2018\" and \"harvested_date\" does not add anything that we are going to use, and we already have a \"post_type\", so we don't need a binary indicator for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_drop_step = first_set_data.drop(columns=[\"retweet\", \"index\", \"new_june_2018\", \"harvested_date\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2) Filtering out the english tweets.\n",
    "After inspecting some of the tweets we concluded that a lot of the language stamps do not match the actual tweets. For instance, most of the tweets labeled to our mothertongue Norwegian was English. The same went for different languages, such as those of neighbouring countries like Danish and Swedish. For this reason we decide to discard all tweets in other languages than english, in order to avoid confusion. We could have included tweets in other languages of which we have some knowledge of, we decide not to as we have limited knowledge of the political climate in these countries. The labeling of english tweets are also better, and our decision also helps keeping the scope of the project limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_filter_english = data1_drop_step.loc[data1_drop_step['language'] == \"English\"]\n",
    "data1_filter_english.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.3) Cleaning empty data and duplicates, plus some small format changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isna_columns = data1_filter_english.isna().any(axis=0)\n",
    "column_nan_list = isna_columns[isna_columns== True].index.tolist()\n",
    "for x in column_nan_list:\n",
    "    print(x, \":\", data1_filter_english[x].isnull().sum())\n",
    "    #clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For content we are going to eliminate the only empty one. For region and account_type, we are going to change it to an string that shows that it is unknown. When the post_type is nan, it only means that it is a original tweet so we change it to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_filter_english[\"account_type\"] = data1_filter_english[\"account_type\"].fillna(\"?\") #gjøre om right til Right?\n",
    "data1_filter_english[\"region\"] = data1_filter_english[\"region\"].fillna(\"Unknown\")\n",
    "data1_filter_english[\"content\"] = data1_filter_english[\"content\"].dropna()\n",
    "data1_filter_english[\"post_type\"] = data1_filter_english[\"post_type\"].fillna(\"ORIGINAL\")\n",
    "#clea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also found out that there are two account_types that are equal. \"Right\" and \"right\". T\n",
    "data1_filter_english[\"account_type\"] = data1_filter_english[\"account_type\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time stamps in this dataset is in string format. For further calculations, it would be better to have them as datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_filter_english[\"publish_date\"] = pd.to_datetime(data1_filter_english[\"publish_date\"], format='%m/%d/%Y\\n%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_dup_step = data1_filter_english.drop_duplicates()\n",
    "duplicates_number = data1_filter_english.shape[0]-data1_dup_step.shape[0] #må endres til data eller noe som er faktisk\n",
    "#drop duplicates in everythin\n",
    "dropdup_specific = data1_filter_english.drop_duplicates(subset= [\"external_author_id\", \"content\", \"following\", \"followers\", \"updates\", \"post_type\"])\n",
    "duplicates_except_timestamp = data1_dup_step.shape[0]-dropdup_specific.shape[0]\n",
    "print(\"Full duplicates:\", duplicates_number, \"\\nDuplicates in all forms except time stamp:\", duplicates_except_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_dropped = data1_filter_english.drop_duplicates(subset= [\"external_author_id\", \"content\", \"following\", \"followers\", \"updates\", \"post_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creators of this dataset also states that they did not sample anything in 2018 so we have to limit our data set to that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_sub_dropped.loc[data_sub_dropped[\"publish_date\"] < \"2018-01-01\"]\n",
    "data1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) The second dataset\n",
    "Twitter released nine million tweets that they had traced to russian troll accounts on 17th October 2018. This dataset was in a much more raw format than the one provided on Kaggle, and consisted of eigth csv-files. Four of them was tweets from the Iranian agency, who also does this on a major scale. The other four was tweets produced by the IRA:\n",
    "- rus_troll_user.csv : Contains user specific features. \n",
    "- rus_troll_tweet_text.csv: Contains text and language of the given tweet.\n",
    "- rus_troll_tweet_metadata.csv: Contains features that are user specific, but may change tweet to tweet.\n",
    "- rus_troll_tweet_stats.csv: Contains other tweet features ## fyll in hva\n",
    "\n",
    "The whole dataset can be downloaded at this link: https://drive.google.com/open?id=1GBsVXYvPrGcYI-wR4mWGO39fly1TMqjO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1) Loading the second dataset\n",
    "We loaded the second dataset in as pandas data frames as well. After that we merged all of them together, wrote the new frame as a parquet file and then read that as a spark file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_new = ZipFile(\"New_russian_tweets.zip\")\n",
    "new_text = pd.read_csv(zip_file_new.open(\"rus_troll_tweet_text.csv\"))\n",
    "new_metadata = pd.read_csv(zip_file_new.open(\"rus_troll_tweet_metadata.csv\")).drop(columns=[\"latitude\",\"longitude\",\"tweet_client_name\"])\n",
    "new_user = pd.read_csv(zip_file_new.open(\"rus_troll_user.csv\")).drop(columns=[\"user_display_name\",\"user_screen_name\",\"user_profile_description\",\"user_profile_url\"])\n",
    "new_stats = pd.read_csv(zip_file_new.open(\"rus_troll_tweet_stats.csv\")).drop(columns=[\"poll_choices\",\"urls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1) Cleaning, merging and filtering the second dataset\n",
    "\n",
    "The data set did not have suitable column names, so we manually created column names under. We also merged all of the different files together, since we had dropped a bunch of columns which we found uninteresting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_english_text = new_text[new_text[\"tweet_language\"]==\"en\"].drop(columns=[\"tweet_language\"])\n",
    "#To create a data frame that only had english tweets in it.\n",
    "new_metadata.columns=[\"tweetid\",\"following\",\"followers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_step = pd.merge(new_english_text, new_metadata, on='tweetid')\n",
    "new_tweets = pd.merge(merge_step,new_stats.filter([\"tweetid\",\"userid\",\"tweet_time\"]) , on='tweetid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first data set we had a column that showed whether the tweet was original, retweet etc. The second set had not the same, so we wanted to create this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort out retweets from one set\n",
    "retweets = new_stats[new_stats[\"is_retweet\"]==True]\n",
    "#Select retweets in tweet set\n",
    "retweets = new_tweets[new_tweets[\"tweetid\"].isin(retweets[\"tweetid\"].tolist())]\n",
    "#add extra column stating that this is a retweet\n",
    "retweets[\"post_type\"] =\"RETWEET\"\n",
    "#We follow same procedure for quote tweets, after finding tweets with quoted tweetid\n",
    "quote_tweets = new_stats[np.isfinite(new_stats['quoted_tweet_tweetid'])]\n",
    "quote_tweets = new_tweets[new_tweets[\"tweetid\"].isin(quote_tweets[\"tweetid\"].tolist())]\n",
    "#we only regard a retweeted quotetweet as a retweet\n",
    "quote_tweets = quote_tweets[~quote_tweets[\"tweetid\"].isin(retweets[\"tweetid\"].tolist())]\n",
    "quote_tweets[\"post_type\"] = \"QUOTE_TWEET\"\n",
    "unoriginal_tweets = quote_tweets.append(retweets)\n",
    "#we label all other tweets as original tweets\n",
    "orig_tweets= new_tweets[~new_tweets[\"tweetid\"].isin(unoriginal_tweets[\"tweetid\"])]\n",
    "orig_tweets[\"post_type\"] = \"ORIGINAL\"\n",
    "data2 = unoriginal_tweets.append(orig_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset now have the following features: \n",
    "- tweetid | The ID of the tweet.\n",
    "- tweet_text | The text of the tweet \n",
    "- tweet_time | The date and time the tweet was sent\n",
    "- following | The number of accounts the handle was following at the time of the tweet\n",
    "- followers | The number of followers the handle had at the time of the tweet\n",
    "- user_id | The unique ID of the user who posted the tweet.\n",
    "- post_type | Indicates if the tweet was a retweet, original tweet or a quote-tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) An overview of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Enriching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enrich the data set by making some new frames for possible use, where we also are going to add some new features. As follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "There are 2.1 million english tweets in this dataset but only a number of unique usernames. We want to take a look at who the unique authors are, what category they are and how they became so prominent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = data1[[\"external_author_id\", \"author\", \"account_category\"]].drop_duplicates(subset = \"external_author_id\")\n",
    "authors = authors[authors.account_category != \"NonEnglish\"]\n",
    "authors[\"tweet_count\"] = data1.groupby('author')['author'].transform('count')\n",
    "authors[\"peak_followers\"] = data1.groupby(['author'], sort=True)['followers'].transform(\"max\")\n",
    "#authors[\"original_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_authors = authors.sort_values(by = \"peak_followers\", axis = 0)\n",
    "most_pop_authors.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_active_authors = authors.sort_values(by = \"tweet_count\", axis = 0).drop_duplicates(subset = [\"author\", \"account_category\", \"tweet_count\", \"peak_followers\"])\n",
    "#For some reason, the author with the most tweets appeared with two different author IDs, and every other column was equal.\n",
    "most_active_authors.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data sets tailored for specific themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_political = data1[data1[\"account_category\"].isin([\"RightTroll\",\"LeftTroll\",\"NewsFeed\"])]\n",
    "data1_political"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frames with different time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first person that entered and started the primaries was Ted Cruz who officially entered the campaign on th 23th of March 2015. Hillary joined in as the first Democratic candidat on 12th April 2015. Donald Trump announced his candidacy on 16th June 2015.\n",
    "\n",
    "Therefore it is interesting to look at the amount of tweets from each side, starting with Ted Cruz' announcement. We have been looking into which party that has seen the most meddling, building up to the first primaries, the period between primaries and election and after the election. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_primaries1 = data1.loc[data1[\"publish_date\"] < \"2016-06-14\"]\n",
    "before_primaries1 = before_primaries1.loc[before_primaries1[\"publish_date\"] > \"2015-03-23\"]\n",
    "before_primaries_time1 = before_primaries1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()\n",
    "#left\n",
    "before_primaries_left1 = before_primaries1.loc[before_primaries1[\"account_type\"] == \"left\"]\n",
    "bp_left_count1 = before_primaries_left1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()\n",
    "#right\n",
    "before_primaries_right1 = before_primaries1.loc[before_primaries1[\"account_type\"] == \"right\"]\n",
    "bp_right_count1 = before_primaries_right1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the primaries, things changed. Now it was a run between Clinton and Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_primaries1 = data1.loc[data1[\"publish_date\"] > \"2016-06-14\"]\n",
    "after_primaries_time1 = after_primaries1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()\n",
    "#left\n",
    "after_primaries_left1 = after_primaries1.loc[after_primaries1[\"account_type\"] == \"left\"]\n",
    "ap_left_count1 = after_primaries_left1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()\n",
    "#right\n",
    "after_primaries_right1 = after_primaries1.loc[after_primaries1[\"account_type\"] == \"right\"]\n",
    "ap_right_count1 = after_primaries_right1[\"publish_date\"].map(lambda x: str(x)[:7]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Key takeaways from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Tweets over time\n",
    "How many tweets have been tweeted each month since the first russian troll tweet? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_axises(title, xlab, ylab):\n",
    "    plt.title(title, size=22), plt.xlabel(xlab, size=20), plt.ylabel(ylab, size=20)\n",
    "# A function for future plotting to save some lines.\n",
    "\n",
    "data1_timesorted = data1.sort_values(\"publish_date\")\n",
    "data1_month_days = data1_timesorted[\"publish_date\"].map(lambda x: str(x)[:7])\n",
    "#if we want only month and days. \n",
    "data1_month_count = data1_month_days.value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(18,7))\n",
    "over_time_fig = plt.bar(data1_month_count.index, data1_month_count, color = \"#3CB371\", align=\"center\")\n",
    "ax.set_xticklabels(data1_month_count.sort_index().index, rotation=90)\n",
    "name_axises(\"Total amount of tweets during the whole data set's time span\", \"Time\", \"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows that there were almost no activity until the announcement of the first candidacies in the primaries, with the first visible chart in november 2014. It is difficult to say whether this represents all the russian troll tweets in this period can amount for the same activity, especially when it comes to the final months of the sample. It could still be viewed as a trend, and we look forward to compare this to the second dataset for the final report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to have a look at each month as well. Which period of the year has spanned the most activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort out the statistics for each month.\n",
    "data1_months = data1_timesorted[\"publish_date\"].map(lambda x: str(x)[5:7])\n",
    "data1_only_months = data1_months.value_counts().sort_index()\n",
    "\n",
    "figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(data1_only_months.index, data1_only_months, color = \"#3CB371\", align=\"center\", zorder=3)\n",
    "plt.grid(zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount of tweets of each month does not change much except small a small increase in the summer period. This is mostly because of the high summer peak in 2017 as can be viewed on the graph showing the tweets over the total time span of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the spread of tweets throughout a day? May it be possible to detect some irregularities regarding the time differences between Russia and USA? We asked the creators of the dataset who told us that the time stamps are all set in the US time zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_time_of_day = data1_timesorted[\"publish_date\"].map(lambda x: str(x)[11:13])\n",
    "data1_only_days = data1_time_of_day.value_counts().sort_index()\n",
    "\n",
    "figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(data1_only_days.index, data1_only_days, color = \"#3CB371\", align=\"center\", zorder=3)\n",
    "plt.grid(zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the russians have done a good job in timing the tweets. The tweets follows an almost perfect frequency regarding to when people are awake, and the majority coming in the evening(here it is necessary to think of the four different time zones in the US). The amount of tweets in the morning may be a bit high, but people would not question this too much if some of the goals are to mimic excessively tweeting political activists or spammers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.x Different user and category stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a look at the users that got a foothold on the app and got the most followers. What kind of category has each user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(most_pop_authors.tail(10))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(13,6))\n",
    "\n",
    "sns.barplot(x=\"author\", y=\"peak_followers\", data=most_pop_authors.tail(10))\n",
    "ax.set_xticklabels(most_pop_authors.tail(10)[\"author\"], rotation=70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of the accounts had a huge amount of followers. In this scale it is fully possible to be viewed as a serious tweeter. The author CRYSTAL1JOHNSON for example is a LeftTroll that was frequently retweeted, for example also by Twitter's own creator, Jack Dorsey (http://uk.businessinsider.com/twitter-ceo-jack-dorsey-retweets-russian-trolls-2018-6?r=US&IR=T). This proves that the trolls can have serious influence on social media. We can also see that many of the top 10 most followed users are RightTroll, which means that this category easily has been passed on as trustworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to take a look at the accounts that has tweeted the most. What kind of category has each user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(most_active_authors.tail(10))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(13,6))\n",
    "\n",
    "sns.barplot(x=\"author\", y=\"tweet_count\", data=most_active_authors.tail(10))\n",
    "ax.set_xticklabels(most_active_authors.tail(10)[\"author\"], rotation=70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that some of the top accounts tweets in enormous scale. Mostly of these are NewsFeed, spreading news disguised as some local institution. This may be to invoke doubt about whether social media news channels can be seen as trustworhty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Looking into specific research questions\n",
    "We found out that many of our initial research questions was harder to answer than initially thought, and that is further descripted in part 4 of this notebook. \n",
    "\n",
    "Questions we left out:\n",
    "- Does the trolls advocate for a common political stance in each specific country? If so which leaning do they have? If not, how polarized are the tweets between left leaning and right leaning?\n",
    "\n",
    "Questions moved to the next milestone (reasons mentioned in part 4): \n",
    "- Is there a way for people without a technical background to determine if a tweet is coming from a Russian troll?\n",
    "- Which themes does the propaganda mainly revolve around? About which issues should people be particularly careful not to believe everything they read? \n",
    "\n",
    "Questions answered:\n",
    " \n",
    "- Were the trolls united with a common political leaning in the period after the primaries in the US elections?\n",
    " \n",
    "- Was the original mission of the Russian trolls for the US election to make sure that Clinton was not elected, or to get Trump elected?\n",
    " \n",
    "- Are the trolls organized as a unit? Do they interact with each other (retweets, etc)?\n",
    "\n",
    "New questions:\n",
    "\n",
    "- How does the regions influence the different types of meddling?\n",
    "  - We wanted to have a look at how the different political leanings were in the different regions. Is there any sort of tactic to each place? Mind looking at how the regions are classified in part 1.1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 How does the regions influence the different types of meddling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to find how the political leanings of the trolls changed by region of operation. We emphasize that our political labels only take english tweets into account. If not there would likely be several extra countries represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We count our tweets by with account type as column, and countries as rows\n",
    "categories_country=pd.DataFrame(data1_political.groupby([\"region\",\"account_category\"]).size()).unstack(level=-1)\n",
    "categories_country.columns = categories_country.columns.droplevel()\n",
    "#We add an extra row with the total amount of each account type\n",
    "#reorder rows\n",
    "categories_country=categories_country.filter(['RightTroll','LeftTroll',\"NewsFeed\"], axis=1)\n",
    "#an extra column is added to sort out countries with to few political tweets\n",
    "categories_country['Total'] = categories_country.sum(axis=1)\n",
    "categories_country = categories_country[categories_country[\"Total\"]>400]\n",
    "categories_country.loc['Total']= categories_country.sum()\n",
    "display(categories_country)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from our dataframe that the US account for an enormous majority of the political tweets. We also see that there are a lot of political tweets with unknown origin. These two rows nearly account for the entire political dataset, while tweets originating from Canada and UK account for less then 1% combined. \n",
    "\n",
    "In order to take a closer look at the leaning of each region we plot the distribution as a stacked bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_country_distribution = categories_country.loc[:,\"RightTroll\":\"NewsFeed\"].div(categories_country[\"Total\"], axis=0)\n",
    "categories_country_distribution.plot.bar(stacked=True)\n",
    "name_axises(\"Distribution Per Region\",\"Regions\",\"Percentage per category\")\n",
    "plt.legend(loc=4)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution of the tweets clearly differ from country to country. An example of this is that none of the Canadian tweets are about news, while nearly all of those of the UK are labeled as NewsFeed. We find this very interesting, as most of the newsfeed articles link to pro-Russia articles, which could shape public opinion on countries foreign policies towards Russia. For this reason we would expect some tweets of this kind to be directed towards Canadians. We also would expect more left/right leaning trolls to tweet in the UK in order to take advantage of the political instability leading towards, and in the aftermath of, the Brexit referendum. Furthermore we see that tweets from the USA are way more evenly spread out among the categories than the other represented countries, although it is clear that the majority of tweets are labeled as \"NewsFeed\" and that the right leaning trolls outnumber the left trolls. Strangely nearly all of the tweets with an unknown origin are right leaning trolls. We find it strange that these tweets do not follow a distribution similar to the overall distribution of the entire set. A reason could be that these accounts find it particularly important not to be geolocated and have taken measures for this. We hope to look further into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"in_reply_to_userid\",\"tweetid\"]\n",
    "replies = new_stats[cols].dropna()\n",
    "users = new_user[\"userid\"].dropna().to_frame()\n",
    "retweets_within = pd.merge(users,replies,how=\"inner\",left_on=\"userid\",right_on=\"in_reply_to_userid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_within = pd.merge(retweets_within,data2,how=\"inner\",left_on=\"tweetid\",right_on=\"tweetid\")\n",
    "#This causes a drop from 190000 retweets to 28606, nearly all communication is non english\n",
    "len(retweets_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Did the troll activity change in regards to the american primaries?\n",
    "\n",
    "This was one of our inital research questions. How did the activity change throughout timeline of the political landscape in the US. We are going to look at two different time periods: Before Clinton and Trump was elected as each of the two parties candidates, and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before primaries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(before_primaries_time1.sort_index(), color = \"#000000\", zorder=3)\n",
    "plt.plot(bp_left_count1.sort_index(), color = \"#DE3A3A\",zorder=3)\n",
    "plt.plot(bp_right_count1.sort_index(), color = \"#3549FD\",zorder=3)\n",
    "plt.grid(zorder=0)\n",
    "name_axises(\"Tweets per month\", \"Time\", \"Number of tweets\")\n",
    "\n",
    "figure(num=None, figsize=(15, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(before_primaries_time1.index, before_primaries_time1, color = \"#000000\", zorder=3)\n",
    "plt.bar(bp_left_count1.index, bp_left_count1, color = \"#DE3A3A\", zorder=3)\n",
    "plt.bar(bp_right_count1.index, bp_right_count1, color = \"#3549FD\", zorder=3)\n",
    "plt.grid(zorder=0)\n",
    "name_axises(\"Tweets per month\", \"Time\", \"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things really started when the first people startet announcing their candidacy. In the beginning, after the first announced candidacy, it is almost an exponential growth of tweets. We can also see that it is a lot of tweets that are not political categorized as well. Let's have a quick look at what happened in the month Hillary announced her candidacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_month1 = data1.loc[data1[\"publish_date\"] > \"2015-04\"]\n",
    "hillary_month1 = hillary_month1.loc[hillary_month1[\"publish_date\"] < \"2015-05\"]\n",
    "hm_count1 = hillary_month1[\"publish_date\"].map(lambda x: str(x)[8:10]).value_counts()\n",
    "#left\n",
    "hm_left1 = hillary_month1.loc[hillary_month1[\"account_type\"] == \"left\"]\n",
    "hm_left_count1 = hm_left1[\"publish_date\"].map(lambda x: str(x)[8:10]).value_counts()\n",
    "#right\n",
    "hm_right1 = hillary_month1.loc[hillary_month1[\"account_type\"] == \"right\"]\n",
    "hm_right_count1 = hm_right1[\"publish_date\"].map(lambda x: str(x)[8:10]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(15, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(hm_count1.sort_index(), color = \"#000000\", label = \"Total tweets\", zorder=3)\n",
    "plt.plot(hm_left_count1.sort_index(), color = \"#DE3A3A\",zorder=3)\n",
    "plt.plot(hm_right_count1.sort_index(), color = \"#3549FD\",zorder=3)\n",
    "plt.grid(zorder=0)\n",
    "name_axises(\"Tweets per day the month Hillary announced her candidacy\", \"Time\", \"Number of tweets\")\n",
    "\n",
    "figure(num=None, figsize=(15, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(hm_count1.index, hm_count1, color = \"#000000\", zorder=3)\n",
    "plt.bar(hm_left_count1.index, hm_left_count1, color = \"#DE3A3A\",zorder=3)\n",
    "plt.bar(hm_right_count1.index, hm_right_count1, color = \"#3549FD\",zorder=3)\n",
    "plt.legend([\"j\"])\n",
    "plt.legend([\"g\"])\n",
    "plt.grid(zorder=0)\n",
    "name_axises(\"Tweets per day the month Hillary announced her candidacy\", \"Time\", \"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount of tweets still are in a small scale compared to future results. Hillary's announcement in itself did not spark significant amount of tweets, but we can see that the number of tweets with a right leaning slowly increased as the primaries started to establish itself in the news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After primaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,8))\n",
    "plt.plot(after_primaries_time1.sort_index(), color = \"#000000\",zorder=3)\n",
    "plt.plot(ap_left_count1.sort_index(), color = \"#DE3A3A\",zorder=3)\n",
    "plt.plot(ap_right_count1.sort_index(), color = \"#3549FD\",zorder=3)\n",
    "plt.grid(zorder=0)\n",
    "ax.set_xticklabels(after_primaries_time1.sort_index().index, rotation=90)\n",
    "name_axises(\"Tweets per month\", \"Time\", \"Number of tweets\")\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1, figsize=(15,8))\n",
    "plt.bar(after_primaries_time1.index, after_primaries_time1, color = \"#000000\", zorder=3)\n",
    "plt.bar(ap_left_count1.index, ap_left_count1, color = \"#DE3A3A\", zorder =3)\n",
    "plt.bar(ap_right_count1.index, ap_right_count1, color = \"#3549FD\", zorder =3)\n",
    "plt.grid(zorder=0)\n",
    "ax1.set_xticklabels(after_primaries_time1.sort_index().index, rotation=90)\n",
    "name_axises(\"Tweets per month\", \"Time\", \"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding to our research question we can say that the primaries actually started the whole thing. At this point, the IRA decided to turn their heads towards american politics. Interestingly enough, we can see that leftist tweets actually dominated until May 2017, and after that it has almost only been the activity of right trolls. Before the primaries the right tweeters were the dominating ones. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Were they making sure Trump won, or making sure Hillary Lost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only regard data before election\n",
    "political_before_election = data1_political[data1_political[\"publish_date\"]<\"2016-11-09\"]\n",
    "#we try to filter out tweets talking about both candidates\n",
    "clinton_data = political_before_election[political_before_election['content'].str\n",
    "                            .contains(\"hillary|clinton|democrat|emails|benghazi|crooked|lock\",na=False,case=False)]\n",
    "trump_data = political_before_election[political_before_election['content'].str\n",
    "                          .contains(\"trump|maga| donald |pence|draintheswamp|republican\",na=False,case=False)]\n",
    "#add extra column and make new dataframe\n",
    "trump_data[\"candidate\"]=\"Trump\"\n",
    "clinton_data[\"candidate\"]=\"Clinton\"\n",
    "candidate_data = trump_data.append(clinton_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have an array of tweets mentioning candidate themed words we display the count of each of them in a bar chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_grouped = candidate_data.groupby(\"candidate\").size().to_frame()\n",
    "candidate_grouped.columns =[\"Tweets mentioning\"]\n",
    "candidate_grouped.plot.bar(figsize=(9,5))\n",
    "name_axises(\"Tweetcount Per Candidate\",\"Candidate\",\"Count\")\n",
    "plt.legend(loc=4)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both candidates are mentioned a lot. Although there is a slight majority of tweets mentioning trump. This does not say much about the preference towards either of the candidates. It does however, imply that both candidate were a major focus of the Russian trolling activity.\n",
    "\n",
    "To get a better sense of the trolls opinions on the candidates, we group by the tweets mentioning each candidate by the account categories. We assume that left trolls have a negative view of Donald Trump, and Right Trolls have a negative view of Hillary Clinton. It could be the case that LeftTrolls have a favorable view of Clinton, although these tweets mentioning her might be to favor other democratic candidates. The same goes for Donald Trump. In order to minimize this uncertainty we only visualize data between the main election and the primaries. We assume the NewsFeed tweets mentioning the candidates have a leaning, but as we cannot say for sure. We disregard these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter away tweets befor the primaries\n",
    "candidate_between = candidate_data[candidate_data[\"publish_date\"]>\"2016-6-14\"]\n",
    "#Group by account category and put the candidate mentioned as columns\n",
    "mentions_per_category = candidate_between.groupby([\"account_category\",\"candidate\"]).size().to_frame().unstack(level=-1)\n",
    "mentions_per_category.columns = mentions_per_category.columns.droplevel()\n",
    "#We drop tweets about newsarticles\n",
    "mentions_per_category.drop(\"NewsFeed\", inplace=True)\n",
    "mentions_per_category.plot.bar(stacked=True, figsize =(8,6))\n",
    "name_axises(\"Candidate Tweets Per Category\",\"Category\",\"Count\")\n",
    "plt.legend(loc=4)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a great majority of the Russian tweets are categorized as \"RightTroll\". This obviously indicate that they wanted a specific outcome. It is however, strange that a not unsignificant amount of tweets in this period is categorized as LeftTroll, as this suggests that tweets sent by the Russian trolls favored Hillary over Trump. From our chart it is also clear that both sides mention both candidates a lot. Strangely, there is a counterintuitive distribution between the candidates for both categories. We see that a majority of left leaning tweets are mentioning Trump, while the right leaning tweets mostly relate to Hillary. This Indicates that it is easier to spread hate and distrust towards a candidate than trust and love. This could also explain why there are left leaning trolls mentioning Trump, as this could be means to help stir up anger among the american population, and destabilize the political climate in the country.\n",
    "\n",
    "We are also curious of how many tweets in the dataset mention each candidate as this could give us a sense of to which degree both candidates were part of the agenda for the trolls on either side of the political spectrum. In this case we look at all tweets before the election. To take a look at the agenda overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tweets not mentioning either candidate, get number of such tweets per category\n",
    "non_candidate_tweets = political_before_election[~political_before_election[\"content\"].isin(candidate_data[\"content\"].tolist())]\n",
    "non_candidate_category = non_candidate_tweets.groupby(\"account_category\").size()\n",
    "#Get amount of tweets by category and candidate\n",
    "total_pol =candidate_data.groupby([\"account_category\",\"candidate\"]).size().to_frame().unstack(level=-1)\n",
    "total_pol.columns = total_pol.columns.droplevel()\n",
    "#Add extra column with amount of tweets not mentioning candidates\n",
    "total_pol[\"Neither\"]=non_candidate_category\n",
    "#Drop newsfeed\n",
    "total_pol.drop(\"NewsFeed\", inplace=True)\n",
    "#plot\n",
    "total_pol = total_pol.div(total_pol.sum(axis=1), axis=0)\n",
    "total_pol.plot.bar(stacked=True)\n",
    "name_axises(\"Distribution Tweets Per Category\",\"Category\",\"Distribution\")\n",
    "plt.legend(loc=1)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a way higher focus from the right leaning side on the candidates themselves. While the left have a way higher representation of tweets not mentioning either of the candidates. This could indicate that the left side were mostly focused on spreading political discourse, or maybe focusing on other candidates in order to divide this side. For the right side however, there are a lot more tweets mentioning the candidates. This indicates a lot more oriented strategy towards the actual result of the election than the left side. We find it interesting that such a high proportion of the right leaning tweets do not mention either candidates on the right side, as the goal of the meddling  supposedly was to influence the outcome of the election."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.x Are the trolls interacting with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to answer this question we wish to inspect the content of the tweets to see if the trolls write the same tweets. If this is the case it is very likely that they operate as a unit with specific messages to share.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tweets with exact same content by the same author\n",
    "without_same_tweet_author = data1.filter([\"external_author_id\",\"content\",\"post_type\"]).drop_duplicates()\n",
    "#Count occurences of each tweet content and sort\n",
    "content_df = without_same_tweet_author.groupby(\"content\").size().to_frame()\n",
    "content_df.columns=[\"count\"]\n",
    "content_df = content_df.sort_values(\"count\",ascending=False)\n",
    "#display findings\n",
    "display(content_df.head(5))\n",
    "#plot the tweetcount distribution in a histogram\n",
    "content_df.hist(bins=20, figsize=(8,6), xlabelsize=15, ylabelsize=15, log =True)\n",
    "name_axises(\"Number of identical tweets by tweetcount\",\"Tweet count\",\"Number of tweets\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are clearly several tweets that have been tweeted several times by several people. However according to the distribution these are in an overwhelming minority compared to tweets only posted a small number of times. This indicates that they are organized to some extent, as they do have a lot of tweets with the same content independent of each other, although this is in only a small extent compared to the overall tweeting.\n",
    "\n",
    "We are curious to see if these tweets are mainly original tweets, or if the case is that they mainly retweet the same tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of the same content based with post type on columns\n",
    "with_post_type = without_same_tweet_author.groupby([\"content\",\"post_type\"]).size().to_frame().unstack(level=-1)\n",
    "with_post_type.columns = with_post_type.columns.droplevel()\n",
    "#sort by common retweet and display, then do same for original tweets\n",
    "with_post_type = with_post_type.sort_values(\"RETWEET\",ascending=False)\n",
    "display(with_post_type.head(10))\n",
    "with_post_type = with_post_type.sort_values(\"ORIGINAL\",ascending=False)\n",
    "display(with_post_type.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the two sorted frames that most of the tweets that occur many times are retweets. This makes sense as a retweet is by design the same tweet. We also see that the majority of the \"original\" tweets in the set start with \"RT\" meaning that the given tweet is actually a retweet. All this indicates that the trolls mainly avoid writing the same things. They do not shy away from retweeting the same things though. We find it interesting that the most common content of the trolls tweets is original tweets containing a quite complicated string that have a low likelyhood of occuring by coincidence. This could indicate that the trolls at times use bots, or that lots of accounts are managed by the same person, that merely copy the same string into the tweet. \n",
    "\n",
    "In order to take a look at weether they run bots, we remove the links in the tweets and sort. If the amount of tweets with the same content increase drastically this is an indication that they run bots that re-use the description of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we strip data on https to remove links and only keep start of tweet\n",
    "stripped_data = without_same_tweet_author\n",
    "stripped_data[\"content\"]=stripped_data[\"content\"].apply(lambda s: str(s).split(\"https\")[0].strip())\n",
    "#we sort data by contents of the remaining part of the tweet. We then remove the top entry as this is an empty string.\n",
    "stripped_data = stripped_data.groupby([\"content\"]).size().sort_values(ascending=False).to_frame().iloc[1:]\n",
    "stripped_data.columns=[\"count\"]\n",
    "#we display the resulting tweets sorted by tweet count\n",
    "display(stripped_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a lot more tweets with the same content when we remove the containing web links. This indicates heavy bot use by the trolls. In order to compare the difference we plot the distributions of tweetcount side by side, before and after removing links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.hist(bins=20, figsize=(8,6), xlabelsize=15, ylabelsize=15, log =True)\n",
    "name_axises(\"Before removal\",\"Tweet count\",\"Number of tweets\")\n",
    "plt.legend(loc=1)\n",
    "\n",
    "stripped_data.hist(bins=20, figsize=(8,6), xlabelsize=15, ylabelsize=15, log =True)\n",
    "name_axises(\"After removal\",\"Tweet count\",\"Number of tweets\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution is clearly skewed towards more tweets with the same content. We conclude that the bots are organized to some extent by using bots etc. We do however suspect that they are run by the same people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the trolls retweeting each other ---- spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure this out, we take all retweeted tweets and join them with the set of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2.to_parquet('data2.parquet.gzip', compression='gzip')\n",
    "#new_stats.to_parquet('stats.parquet.gzip', compression='gzip')\n",
    "#new_tweets.to_parquet('tweets.parquet.gzip', compression='gzip')\n",
    "#new_text.to_parquet('text.parquet.gzip', compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = spark.read.parquet('tweets.parquet.gzip')\n",
    "text = spark.read.parquet('text.parquet.gzip')\n",
    "stats = spark.read.parquet('stats.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9041308\n",
      "3261931\n"
     ]
    }
   ],
   "source": [
    "print(text.count())\n",
    "print(text.filter(\"tweet_language = 'en'\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = stats.where(col(\"in_reply_to_tweetid\").isNotNull())\\\n",
    "        .select(\"in_reply_to_tweetid\",col(\"tweetid\").alias(\"replyid\"))\n",
    "replies =  replies.join(text, text.tweetid == replies.replyid)\\\n",
    "            .select(\"in_reply_to_tweetid\",\"replyid\",col(\"tweet_text\").alias(\"reply_text\"))\n",
    "replies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_within = text.join(replies, text.tweetid == replies.in_reply_to_tweetid)\\\n",
    "            .select(\"tweet_text\",\"tweetid\",\"replyid\",\"reply_text\",\"tweet_language\")\n",
    "print(replies_within.count())\n",
    "print(replies_within.filter(\"tweet_language = 'en'\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets = (stats.where(col(\"retweet_tweetid\").isNotNull())).withColumnRenamed('tweetid','tweet_id')\n",
    "retweets = retweets.join(text, text.tweetid == retweets.tweet_id)\\\n",
    "            .select(\"retweet_tweetid\",\"retweet_userid\",\"userid\",\"retweet_tweetid\",col(\"tweetid\").alias(\"replyid\"))\n",
    "retweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900183\n",
      "137853\n"
     ]
    }
   ],
   "source": [
    "retweets_within = text.join(retweets, text.tweetid == retweets.retweet_tweetid)\\\n",
    "            .select(\"tweet_text\",\"retweet_tweetid\",\"replyid\",\"tweet_language\",\"retweet_userid\",\"userid\")\n",
    "print(retweets_within.count())\n",
    "print(retweets_within.filter(\"tweet_language = 'en'\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted_users = retweets_within.groupBy(\"retweet_userid\").agg(count('*')).\\\n",
    "                select(\"retweet_userid\", col(\"Count(1)\").alias(\"retweets_by_trolls\")).sort(desc(\"retweets_by_trolls\"))\n",
    "retweeted_users.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retweeted_users.count())\n",
    "print(retweeted_users.filter(\"retweets_by_trolls >= 10\").count())\n",
    "print(retweeted_users.filter(\"retweets_by_trolls >= 100\").count())\n",
    "print(retweeted_users.filter(\"retweets_by_trolls >= 1000\").count())\n",
    "print(retweeted_users.filter(\"retweets_by_trolls >= 10000\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              userid|retweets_of_trolls|\n",
      "+--------------------+------------------+\n",
      "|          3438999494|              5426|\n",
      "|0994abf9fb8fe1bf6...|              4615|\n",
      "|b20786822be5f0794...|              4378|\n",
      "|          1687183549|              4016|\n",
      "|c465bceee4e65cc39...|              3810|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeting_users = retweets_within.groupBy(\"userid\").agg(count('*')).\\\n",
    "                select(\"userid\", col(\"Count(1)\").alias(\"retweets_of_trolls\")).sort(desc(\"retweets_of_trolls\"))\n",
    "retweeting_users.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2318\n",
      "1197\n",
      "1197\n",
      "+--------------------+------------------+\n",
      "|               usrid|retweets_of_trolls|\n",
      "+--------------------+------------------+\n",
      "|          3438999494|              5426|\n",
      "|0994abf9fb8fe1bf6...|              4615|\n",
      "|b20786822be5f0794...|              4378|\n",
      "|          1687183549|              4016|\n",
      "|c465bceee4e65cc39...|              3810|\n",
      "|14882528f53ed4b8f...|              3707|\n",
      "|c9da6f87d9766373d...|              3417|\n",
      "|1a73e268f2d87d6d0...|              3378|\n",
      "|          3312143142|              3377|\n",
      "|d4a474a1c8d2e1b7a...|              3372|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(retweeting_users.count())\n",
    "most_retweeting_users = retweeting_users.filter(\"retweets_of_trolls >= 100\").withColumnRenamed(\"userid\",\"usrid\")\n",
    "print(most_retweeting_users.count())\n",
    "print(retweeting_users.filter(\"retweets_of_trolls >= 100\").count())\n",
    "most_retweeting_users.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_most_retweeting_users = most_retweeting_users.withColumnRenamed(\"userid\",\"usrid\")\n",
    "tweets_by_most_retweeting_users = tweets_by_most_retweeting_users\\\n",
    "                                .join(stats, stats.userid == most_retweeting_users.usrid)\\\n",
    "                        .select(\"userid\",\"tweetid\")\n",
    "tweets_by_most_retweeting_users = tweets_by_most_retweeting_users.groupBy(\"userid\").agg(count('*')).\\\n",
    "                select(\"userid\", col(\"Count(1)\").alias(\"total_tweets\")).sort(desc(\"total_tweets\"))\n",
    "tweets_by_most_retweeting_users = tweets_by_most_retweeting_users.join(most_retweeting_users,\n",
    "                                                                       tweets_by_most_retweeting_users.userid ==\n",
    "                                                                      most_retweeting_users.usrid)\\\n",
    "                                    .select(\"userid\",\"total_tweets\",\"retweets_of_trolls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+------------------+\n",
      "|              userid|total_tweets|retweets_of_trolls|        Percentage|\n",
      "+--------------------+------------+------------------+------------------+\n",
      "|9bd87edcd300b276a...|         119|               118|0.9915966386554622|\n",
      "|9304dbfe749ac1b87...|         106|               103|0.9716981132075472|\n",
      "|ed18ded2abb68f0bc...|         116|               112|0.9655172413793104|\n",
      "|9f08edeb78f6dee82...|         110|               106|0.9636363636363636|\n",
      "|1bed07f5349b6e10b...|         108|               104|0.9629629629629629|\n",
      "|dfee299ad7755c7ac...|         107|               103|0.9626168224299065|\n",
      "|9d1a1ff4e8016ea1d...|         122|               117|0.9590163934426229|\n",
      "|179019db26001208f...|         124|               118|0.9516129032258065|\n",
      "|f81678fb42a6bae9c...|         121|               112|0.9256198347107438|\n",
      "|4584ea5091944dd0d...|         125|               114|             0.912|\n",
      "|7a139f4631e610230...|         118|               104|0.8813559322033898|\n",
      "|9d6cca646e3e183e2...|         146|               114|0.7808219178082192|\n",
      "|e29cf564295188f03...|         322|               249|0.7732919254658385|\n",
      "|44d631516b64aaaf9...|        1056|               811|0.7679924242424242|\n",
      "|726a39d0e2547bf4f...|         155|               116|0.7483870967741936|\n",
      "|49f9357f85ba8a69f...|         307|               226|0.7361563517915309|\n",
      "|7b9567b5a07873c7c...|         249|               180|0.7228915662650602|\n",
      "|b01886e1288f7648a...|         203|               140|0.6896551724137931|\n",
      "|cd9204fca7692f952...|         193|               133| 0.689119170984456|\n",
      "|21d94530a89f07224...|         876|               576|0.6575342465753424|\n",
      "+--------------------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_by_most_retweeting_users = tweets_by_most_retweeting_users.withColumn(\n",
    "                                \"Percentage\", (col(\"retweets_of_trolls\") /col(\"total_tweets\"))).sort(desc(\"Percentage\"))\n",
    "\n",
    "                \n",
    "tweets_by_most_retweeting_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_within.groupBy(\"tweet_language\").agg(count('*')).sort(desc(\"Count(1)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.count())\n",
    "text.groupBy(\"tweet_language\").agg(count('*')).sort(desc(\"Count(1)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_within.groupBy(\"tweet_text\").agg(count('*')).sort(desc(\"Count(1)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_within.groupBy(\"retweet_tweetid\").agg(count('*')).sort(desc(\"Count(1)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equality between the above dataframes indicate that they dont use bots to retweet everyone with the same text etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_within.groupBy(\"tweet_text\").agg(count('*')).sort(desc(\"Count(1)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most of the replied to tweets are written in \"kiryllisk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_within.groupBy(\"reply_text\").agg(count('*')).sort(desc(\"Count(1)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_text_grouped = tweets.join(retweets, tweets.tweetid == stats.in_reply_to_tweetid).select(\"tweet_text\",\"tweetid\",\"userid\")\\\n",
    "            .groupBy(\"tweet_text\").agg(count(\"*\")).sort(desc(\"count(1)\"))\n",
    "\n",
    "retweets_text_grouped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Discussion of our project so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Critique of own analysis and sources of potential error\n",
    "\n",
    "For many of our tasks it is unclear whether one can say that this is an total accurate representation of russian meddling on twitter. We only have a data sample, and it is likely that some of these accounts were taken down, given up on by IRA or by other reasons are not as active through the total period. In, it shows a general trend of the activity.\n",
    "\n",
    "Disclaimer: Many of the visualizations still needs major improvements. Especially part 3.2 and some of the early descriptive statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Own thoughts of milestone 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1) Thoughts regarding the data sets\n",
    "\n",
    "**First dataset:**\n",
    "The first data set had some features that initally was thought to be useful that ended up not being so much, especially region and language. This encoding was not fully correct, and we therefore had to use them much more careful than earlier believed. \n",
    "**Second dataset:**\n",
    "\n",
    "Things got a little changed when we got the possibility to work with a second data set on Sunday 18th November. The new data set featured a bunch of new interesting features, as well as not being categorized in the same manner as the first. We have not been able to look too much into it yet, but we really like the idea of combining the two and then getting more data samples. Right now, the second dataset does not have that many features, but we will be adding some more that we think will be useful for further analysing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2) Dropped ideas or changed approaches to them.\n",
    "**Other languages**\n",
    "\n",
    "We worked a while on trying to analyse some of the non-english tweets, in particular the european ones. It was a significant amount, especially of the german data, but we struggled to get something in particular out of it. In general text classification became harder than initially thought. \n",
    "\n",
    "**General text classification** \n",
    "\n",
    "We was originally going to do this regarding the question how someone could determine whethter a twitter account is a troll. We knew that the TextBlob library had the application to create a sentiment, subjectivity and polarization tweets. We applied this, but when we were visualizing it we found out that it did not work as we had hoped. We found out that many of the tweets were not giving values even though there were clear indication of negative speak or positive speak. We also saw that the library struggled when tweets were particulary short. \n",
    "\n",
    "**Concerning traits to look for whether a user is a troll**\n",
    "\n",
    "We found out that many of the twitter users have irregular rhytms when it comes to followers and the update of them after tweeting. Some had sudden drops of hundreds of followers and following in just a few minutes, which may imply buying of fake or real followers (or own methods of phishing to get them). Others had irregular amounts of tweets in short time span, and also some users were sharing the same rhythm of the same tweets. Unfortunately we did not have time for analysing this in whole before the end of milestone 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Plan for the next milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:\n",
    "- We are not pleased with the visualization at the moment, and want to increase our quality here.\n",
    "- Look for other alternatives than used right now or effectively improve those we already have.\n",
    "\n",
    "Modeling:\n",
    "- Try to create a model that could determine which category each twitter user is. This may be difficult to manage, but the idea is to split the original dataset into a training and test set and create a model that could classify the different categories. Then apply this one the new one and see if it actually has made an improvement on the dataset and could be used as an actual classifier.\n",
    "\n",
    "Perform the analysis regarding our most important research question - \"Is there a way for people without a technical background to determine if a tweet is coming from a Russian troll?\":\n",
    "- Analyse followers and following to look deeper into the movement of this.\n",
    "- Look at the frequency and patterns of the time the tweets were tweeted.\n",
    "\n",
    "Improving the second data set:\n",
    "- Add features similar to the original data set.\n",
    "- Enrich the second data set by creating similar and new subsets.\n",
    "- Look more into the interaction between trolls in the new set, since we also have the user ID of each tweet.\n",
    "\n",
    "Create the data story:\n",
    "- Look for suitable tools to create our data story in.\n",
    "- Extract our most important findings and present something that actually provides meaningful insight to how the russian trolls operates.\n",
    "- Search for and add features unique for the data story. Animations e.g.\n",
    "- Finish the data story.\n",
    "\n",
    "Create the poster:\n",
    "- Sort out the visuals that works best in paper format, and choose the most interesting results for this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
